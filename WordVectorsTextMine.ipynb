{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordVectorsTextMine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ispapadakis/nlp/blob/master/WordVectorsTextMine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I70NXl251Kfh"
      },
      "source": [
        "# Natural Language Processing:<br>\n",
        "Text Sentiment Classification Applied to Short Text\n",
        "\n",
        "Data from Quora Insincere Questions Classification Competition\n",
        "\n",
        "### Features\n",
        "\n",
        "- Text Analysis Using Pretrained Word Vectors\n",
        "- Fit Bidirectional RNN\n",
        "- Apply Custom Keras Loss Function to Maximize F1-Score\n",
        "- Accept Misspellings of Most Common Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5ZPV2uezt1x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8de25502-1584-4797-ffe1-88eb8591afb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eetQKbtvze3L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf02c72d-5a66-4c87-b286-b60f6798618d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import gc\n",
        "import re\n",
        "from datetime import datetime as dt\n",
        "from difflib import get_close_matches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Embedding, concatenate, Dense, Dropout\n",
        "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, Bidirectional, CuDNNGRU\n",
        "from keras.models import Model\n",
        "from keras.initializers import glorot_normal, orthogonal\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "#from keras import initializers, regularizers, constraints, optimizers, callbacks\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzF5WWFLze3P"
      },
      "source": [
        "train = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/train.csv\")\n",
        "test = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lve74PEX2SYV"
      },
      "source": [
        "### Clean Text With Emphasis in Non-English Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOCwdln1ze3R"
      },
      "source": [
        "puncts = list('!\"#$%&\\'()*+,\\-.\\/:;<=>?@\\[\\]\\\\\\\\^_`{|}~')\n",
        "def clean_text(x):\n",
        "\n",
        "    #x = str(x) # needed if x is nan\n",
        "    spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d',\n",
        "                '\\xad', '\\xa0', 'करना', 'है']\n",
        "    for s in spaces:\n",
        "        x = x.replace(s, ' ')\n",
        "\n",
        "    x = re.sub('[’‘´`”“]', \"'\", x)\n",
        "    x = re.sub(' … ', \" ... \", x)\n",
        "    x = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', x)\n",
        "\n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "\n",
        "    return x\n",
        "\n",
        "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))\n",
        "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADh8VwRj2xkP"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6nxToG8ze3T"
      },
      "source": [
        "## Parameters\n",
        "embed_size = 300 # word vector size\n",
        "max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 70 # Maximum message length (ignore after this many words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzvtNDioze3V"
      },
      "source": [
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words = max_features, lower = False, filters = \"\") # Expect that lower case and capitalized have diff meanings\n",
        "tokenizer.fit_on_texts(train[\"question_text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eSyGC2Dze3Z"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(train[\"question_text\"])\n",
        "X_test = tokenizer.texts_to_sequences(test[\"question_text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdUyhh_eze3a"
      },
      "source": [
        "## Pad the sentences\n",
        "X = pad_sequences(X, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj88Avxpze3c"
      },
      "source": [
        "## vector of target values\n",
        "Y = train['target'].values\n",
        "## submissiong data frame\n",
        "sub_ = test[['qid']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1ns-HyKze3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22576d95-8c81-4b21-9ebc-685b4d9fa509"
      },
      "source": [
        "del train, test\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeTtvISEze3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76e8c02b-16a1-457a-c1b7-55317af8dc36"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(\"Word Index Entries: \",len(word_index))\n",
        "lower_case_word = set(t.lower() for t in word_index)\n",
        "max_features = len(word_index) + 1\n",
        "approx = dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index Entries:  240750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_paxQB122tl"
      },
      "source": [
        "### Read Pretrained Word Vectors\n",
        "\n",
        "Use: Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
        "\n",
        "If a word is not found among word vector choices:\n",
        "\n",
        "- Check if different capitalization is available\n",
        "- Check for misspellings of most common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KsvOn1Rze3k"
      },
      "source": [
        "#%%time\n",
        "#glove = pd.read_csv('glove.840B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0, na_filter=False)\n",
        "#CPU times: user 1min 16s, sys: 26.3 s, total: 1min 43s\n",
        "#Wall time: 4min 49s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBJN_pONze3m"
      },
      "source": [
        "def word_vectors(embedding_file):\n",
        "    global approx\n",
        "    embeddings_index = dict()\n",
        "    ignore_case_index = dict()\n",
        "    \n",
        "    # Read Word Vector File\n",
        "    f = open(embedding_file, errors='ignore')\n",
        "    for line in f:\n",
        "        lst = line.split()\n",
        "        if len(lst) != 301: # Some files have headers, ignore them\n",
        "            continue\n",
        "        if lst[0] in word_index: \n",
        "            embeddings_index[lst[0]] = np.asarray(lst[1:], dtype='float32')\n",
        "        if lst[0].lower() in lower_case_word:\\\n",
        "            ignore_case_index[lst[0].lower()] = np.asarray(lst[1:], dtype='float32')\n",
        "    f.close()\n",
        "\n",
        "    # Add words not in word vector keys if lower case of word is in\n",
        "    for word in word_index:\n",
        "        if word not in embeddings_index and word.lower() in ignore_case_index:\n",
        "            embeddings_index[word] = ignore_case_index[word.lower()]\n",
        "\n",
        "    # if a) word is of length > 5 and b) word is among the 100000 most common \n",
        "    #    and c) not in known words, then add to question marks list\n",
        "    qmark = [w for w in word_index if len(w)>5 and word_index[w]<100000 and w not in embeddings_index]\n",
        "    \n",
        "    # if approx dict is not empty\n",
        "    if not approx:\n",
        "        # add to solid words list words of a) length > 5, b) among the 30000 most common, and c) known\n",
        "        solid = [w for w in word_index if len(w)>5 and word_index[w]<30000 and w in embeddings_index]\n",
        "        # find close matches to question mark words among solid words\n",
        "        approx = {w:get_close_matches(w, solid, n=1, cutoff=0.80) for w in qmark}\n",
        "        approx[\"Quorans\"] = \"Quoran\"\n",
        "        print(\"Spelll Checking --> Solids = \", len(solid),\" QMarks = \",len(qmark))\n",
        "        print(\"Corrections = \",sum(bool(v) for v in approx.values()))\n",
        "        \n",
        "    # if there is an approximation for a question mark use its word vector \n",
        "    for w in qmark:\n",
        "        if w in approx and approx[w]:\n",
        "            a = approx[w][0]\n",
        "            if a in embeddings_index:\n",
        "                embeddings_index[w] = embeddings_index[a]\n",
        "\n",
        "    # Calculate center and std for each word vector dimension among known words\n",
        "    all_embs = np.stack(tuple(embeddings_index.values()))\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    #embed_size = all_embs.shape[1]\n",
        "\n",
        "    np.random.seed(2019)\n",
        "    # KEY!! assign random vectors to unknown words\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features:\n",
        "            continue\n",
        "        if word in embeddings_index:\n",
        "            embedding_matrix[i] = embeddings_index[word]\n",
        "            \n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFjyNHGNze3o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "4d189589-3749-4cd3-d27b-e986be0ebc17"
      },
      "source": [
        "%%time\n",
        "\n",
        "embedding_matrix = word_vectors('gdrive/My Drive/Colab Notebooks/glove.840B.300d.txt')\n",
        "embedding_matrix[0,] = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spelll Checking --> Solids =  20112  QMarks =  3050\n",
            "Corrections =  1152\n",
            "CPU times: user 4min 43s, sys: 4.35 s, total: 4min 47s\n",
            "Wall time: 5min 34s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa3UBcEFMZ7s"
      },
      "source": [
        "#[(k,v) for k,v in approx.items() if v]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUt3KtC7ze3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66c5d8a8-8fcd-4610-a26b-06282fe89341"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHADHByPmseo"
      },
      "source": [
        "##### CUSTOM LOSS FUNCTION\n",
        "\n",
        "# Weighted Binary Crossentropy\n",
        "\n",
        "def weighted_binary_crossentropy_init(dev_from_1_weight = 0.5):\n",
        "  \n",
        "  dev_from_0_weight = 1 - dev_from_1_weight\n",
        "  \n",
        "  def wbce_loss(y_true, y_pred):\n",
        "      wce1 = dev_from_1_weight * y_true * K.log( K.clip(y_pred, K.epsilon(), None) )\n",
        "      wce0 = dev_from_0_weight * (1.0-y_true) * K.log( K.clip(1.0-y_pred, K.epsilon(), None) )\n",
        "      return K.mean(-(wce1+wce0), axis=-1)\n",
        "    \n",
        "  return wbce_loss\n",
        "\n",
        "\n",
        "# Negative of F1 Score at Given Threshold\n",
        "\n",
        "def f1_loss(thresh = 0.35, smooth = 1e-5):\n",
        "\n",
        "    def f1_neg(y_true, y_pred):\n",
        "        y_pred = K.sigmoid( 1e6 * (y_pred - thresh))\n",
        "        y_true_f = K.flatten(y_true)\n",
        "        y_pred_f = K.flatten(y_pred)\n",
        "        intersection = K.sum(y_true_f * y_pred_f)\n",
        "        return -(2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "    return f1_neg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cca9-zGqze3s"
      },
      "source": [
        "# Quick Approximation of Max F1-Score\n",
        "def f1_smart(y_true, y_pred):\n",
        "    args = np.argsort(y_pred)\n",
        "    tp = y_true.sum()\n",
        "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
        "    res_idx = np.argmax(fs)\n",
        "    return 2 * fs[res_idx], y_pred[args[res_idx:res_idx+2]].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1ZLPuttwomz"
      },
      "source": [
        "weighted_binary_crossentropy = weighted_binary_crossentropy_init(0.40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYpKZjDDze3t"
      },
      "source": [
        "### Neural Network Configuration\n",
        "\n",
        "def nn_config_a(gru_units=64, dense_units=16, dropout_rate=0.40):\n",
        "    K.clear_session()\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x = Bidirectional(CuDNNGRU(gru_units, return_sequences = True, kernel_initializer = glorot_normal(seed = 12300),\n",
        "                                recurrent_initializer = orthogonal(gain = 1.0, seed = 10000)))(x)\n",
        "    emm_max_pool = GlobalMaxPooling1D()(x)\n",
        "    emm_ave_pool = GlobalAveragePooling1D()(x)\n",
        "    out = concatenate([emm_max_pool, emm_ave_pool])\n",
        "    out = Dense(dense_units, activation = \"relu\", kernel_initializer = glorot_normal(seed = 12300))(out)\n",
        "    out = Dropout(dropout_rate)(out)\n",
        "    out = Dense(dense_units, activation = \"relu\", kernel_initializer = glorot_normal(seed = 12300))(out)\n",
        "    out = Dropout(dropout_rate)(out)\n",
        "    out = Dense(1, activation = \"sigmoid\")(out)\n",
        "    model = Model(inputs = inp, outputs = out)\n",
        "    f1_neg = f1_loss()\n",
        "    model.compile(loss = weighted_binary_crossentropy, optimizer = RMSprop(clipvalue=0.5), metrics = [f1_neg])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb38aAFnze3v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e659ecfa-f2e3-41d1-e380-1c6805d58082"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=True)\n",
        "stats = {\"f1\":dict(), \"cutoff\":dict(), \"pctile\":dict(), \"avg_prob\":dict()}\n",
        "y_test = np.zeros((X_test.shape[0],)).reshape([-1,1])\n",
        "\n",
        "# Cure Schedule\n",
        "p  = {\"batch_size\":512, \"lr1\":[0.0010,0.0010,0.0005,0.0005], \"lr2\":[2e-5]}\n",
        "\n",
        "oof = np.array([[],[],[]])\n",
        "\n",
        "t0 = dt.now()\n",
        "__show_model__ = True\n",
        "for i, (train_index, valid_index) in enumerate(kfold.split(X,Y)):\n",
        "    model = nn_config_a()\n",
        "    if __show_model__:\n",
        "        model.summary()\n",
        "        __show_model__ = False\n",
        "\n",
        "    print(\"\\n\\nFold:\",i)\n",
        "    X_train, X_val, Y_train, Y_val = X[train_index], X[valid_index], Y[train_index], Y[valid_index]\n",
        "    #checkpoint = ModelCheckpoint(\"weights_best.h5\", monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
        "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=1, min_lr=0.0001, verbose=2)\n",
        "    lrate = lambda epoch: p[\"lr1\"][epoch]\n",
        "    lr_schedule = LearningRateScheduler(lrate, verbose=0)\n",
        "    #earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n",
        "    callbacks = [lr_schedule]\n",
        "    model.fit(X_train, Y_train, batch_size=p[\"batch_size\"], epochs=len(p[\"lr1\"]),\n",
        "        validation_data=(X_val, Y_val), verbose=2, callbacks=callbacks)\n",
        "    #model.load_weights(filepath)\n",
        "    print(\"Elapsed Time: \", np.round((dt.now()-t0).seconds / 60,2), \" min\")\n",
        "\n",
        "    for lr_curr in p[\"lr2\"]:\n",
        "        model.layers[1].trainable = True   # Embedding\n",
        "        #model.layers[2].trainable = False   # Bidirectional\n",
        "\n",
        "        f1_neg = f1_loss()\n",
        "        model.compile(loss = weighted_binary_crossentropy, optimizer = RMSprop(lr=lr_curr, clipvalue=0.5), metrics = [f1_neg])\n",
        "        model.fit(X_train, Y_train, batch_size=2048, epochs=1,\n",
        "            validation_data=(X_val, Y_val), verbose=2) #, class_weight = {0:1,1:1.5})\n",
        "\n",
        "        #model.load_weights(filepath)\n",
        "        print(\"Elapsed Time: \", np.round((dt.now()-t0).seconds / 60,2), \" min\")\n",
        "\n",
        "    y_pred = model.predict(X_val, batch_size=1024).flatten()\n",
        "    f1, threshold = f1_smart(Y_val, y_pred)\n",
        "    oof = np.append(oof,[Y_val, y_pred, np.full((X_val.shape[0],),i)], axis = 1)\n",
        "    y_test += model.predict(X_test, batch_size=1024) / 5\n",
        "\n",
        "    print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))\n",
        "    print(\"F1(0.34) = \",f1_score(Y_val,y_pred>=0.34),\"\\n\\n\\n\")\n",
        "    stats[\"f1\"][i] = f1\n",
        "    stats[\"cutoff\"][i] = threshold\n",
        "    stats[\"pctile\"][i] = np.round(np.mean(y_pred >= threshold),5)\n",
        "    stats[\"avg_prob\"][i] = y_pred.mean()\n",
        "\n",
        "print(\"Elapsed Time: \", np.round((dt.now()-t0).seconds / 60,2), \" min\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 70)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 70, 300)      72225300    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 70, 128)      140544      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 16)           4112        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 16)           272         dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            17          dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 72,370,245\n",
            "Trainable params: 144,945\n",
            "Non-trainable params: 72,225,300\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Fold: 0\n",
            "Train on 1044897 samples, validate on 261225 samples\n",
            "Epoch 1/4\n",
            " - 50s - loss: 0.0636 - f1_neg: -5.7764e-01 - val_loss: 0.0531 - val_f1_neg: -6.4614e-01\n",
            "Epoch 2/4\n",
            " - 51s - loss: 0.0551 - f1_neg: -6.4243e-01 - val_loss: 0.0511 - val_f1_neg: -6.6036e-01\n",
            "Epoch 3/4\n",
            "Epoch 4/4\n",
            " - 51s - loss: 0.0491 - f1_neg: -6.8486e-01 - val_loss: 0.0498 - val_f1_neg: -6.6575e-01\n",
            "Elapsed Time:  3.42  min\n",
            "Train on 1044897 samples, validate on 261225 samples\n",
            "Epoch 1/1\n",
            " - 69s - loss: 0.0467 - f1_neg: -7.0602e-01 - val_loss: 0.0495 - val_f1_neg: -6.7157e-01\n",
            "Elapsed Time:  4.58  min\n",
            "Optimal F1: 0.6746 at threshold: 0.2946\n",
            "F1(0.34) =  0.6727337404399123 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold: 1\n",
            "Train on 1044897 samples, validate on 261225 samples\n",
            "Epoch 1/4\n",
            " - 53s - loss: 0.0626 - f1_neg: -5.7906e-01 - val_loss: 0.0503 - val_f1_neg: -6.5064e-01\n",
            "Epoch 2/4\n",
            " - 52s - loss: 0.0536 - f1_neg: -6.3736e-01 - val_loss: 0.0488 - val_f1_neg: -6.3576e-01\n",
            "Epoch 3/4\n",
            " - 52s - loss: 0.0500 - f1_neg: -6.6233e-01 - val_loss: 0.0482 - val_f1_neg: -6.4379e-01\n",
            "Epoch 4/4\n",
            " - 52s - loss: 0.0483 - f1_neg: -6.7303e-01 - val_loss: 0.0482 - val_f1_neg: -6.3280e-01\n",
            "Elapsed Time:  8.22  min\n",
            "Train on 1044897 samples, validate on 261225 samples\n",
            "Epoch 1/1\n",
            " - 70s - loss: 0.0459 - f1_neg: -6.9416e-01 - val_loss: 0.0471 - val_f1_neg: -6.7327e-01\n",
            "Elapsed Time:  9.38  min\n",
            "Optimal F1: 0.6817 at threshold: 0.2411\n",
            "F1(0.34) =  0.675 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold: 2\n",
            "Train on 1044898 samples, validate on 261224 samples\n",
            "Epoch 1/4\n",
            " - 53s - loss: 0.0632 - f1_neg: -5.7903e-01 - val_loss: 0.0529 - val_f1_neg: -6.5207e-01\n",
            "Epoch 2/4\n",
            " - 52s - loss: 0.0554 - f1_neg: -6.4207e-01 - val_loss: 0.0530 - val_f1_neg: -6.0995e-01\n",
            "Epoch 3/4\n",
            " - 52s - loss: 0.0518 - f1_neg: -6.7068e-01 - val_loss: 0.0517 - val_f1_neg: -6.6096e-01\n",
            "Epoch 4/4\n",
            " - 52s - loss: 0.0503 - f1_neg: -6.8017e-01 - val_loss: 0.0501 - val_f1_neg: -6.7426e-01\n",
            "Elapsed Time:  13.02  min\n",
            "Train on 1044898 samples, validate on 261224 samples\n",
            "Epoch 1/1\n",
            " - 69s - loss: 0.0477 - f1_neg: -7.0342e-01 - val_loss: 0.0499 - val_f1_neg: -6.7605e-01\n",
            "Elapsed Time:  14.18  min\n",
            "Optimal F1: 0.6790 at threshold: 0.2970\n",
            "F1(0.34) =  0.6766481101670085 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold: 3\n",
            "Train on 1044898 samples, validate on 261224 samples\n",
            "Epoch 1/4\n",
            " - 53s - loss: 0.0627 - f1_neg: -5.7764e-01 - val_loss: 0.0518 - val_f1_neg: -6.2097e-01\n",
            "Epoch 2/4\n",
            " - 52s - loss: 0.0539 - f1_neg: -6.3650e-01 - val_loss: 0.0487 - val_f1_neg: -6.4037e-01\n",
            "Epoch 3/4\n",
            " - 52s - loss: 0.0502 - f1_neg: -6.6554e-01 - val_loss: 0.0487 - val_f1_neg: -6.5755e-01\n",
            "Epoch 4/4\n",
            " - 52s - loss: 0.0486 - f1_neg: -6.7974e-01 - val_loss: 0.0495 - val_f1_neg: -6.4254e-01\n",
            "Elapsed Time:  17.8  min\n",
            "Train on 1044898 samples, validate on 261224 samples\n",
            "Epoch 1/1\n",
            " - 70s - loss: 0.0462 - f1_neg: -6.9967e-01 - val_loss: 0.0486 - val_f1_neg: -6.6279e-01\n",
            "Elapsed Time:  18.97  min\n",
            "Optimal F1: 0.6750 at threshold: 0.2836\n",
            "F1(0.34) =  0.6665201005025126 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold: 4\n",
            "Train on 1044898 samples, validate on 261224 samples\n",
            "Epoch 1/4\n",
            " - 53s - loss: 0.0623 - f1_neg: -5.7514e-01 - val_loss: 0.0498 - val_f1_neg: -6.4873e-01\n",
            "Epoch 2/4\n",
            " - 52s - loss: 0.0536 - f1_neg: -6.2420e-01 - val_loss: 0.0479 - val_f1_neg: -6.4885e-01\n",
            "Epoch 3/4\n",
            " - 52s - loss: 0.0495 - f1_neg: -6.5331e-01 - val_loss: 0.0493 - val_f1_neg: -6.1647e-01\n",
            "Epoch 4/4\n",
            " - 52s - loss: 0.0479 - f1_neg: -6.6745e-01 - val_loss: 0.0489 - val_f1_neg: -6.3492e-01\n",
            "Elapsed Time:  22.58  min\n",
            "Train on 1044898 samples, validate on 261224 samples\n",
            "Epoch 1/1\n",
            " - 70s - loss: 0.0456 - f1_neg: -6.8875e-01 - val_loss: 0.0471 - val_f1_neg: -6.6912e-01\n",
            "Elapsed Time:  23.77  min\n",
            "Optimal F1: 0.6817 at threshold: 0.2242\n",
            "F1(0.34) =  0.6705333501450006 \n",
            "\n",
            "\n",
            "\n",
            "Elapsed Time:  23.87  min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrKCcO4Bze3x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "327c6cd9-e910-4d0c-e465-a2432c2498fd"
      },
      "source": [
        "print('{:10}'.format(\".\") + ' '.join('Fold {} '.format(i) for i in range(5)))\n",
        "for m in stats:\n",
        "    out = '{:9}'.format(m)\n",
        "    out += \" \".join('{:7.4f}'.format(stats[m][i]) for i in range(5) if i in stats[m])\n",
        "    print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".         Fold 0  Fold 1  Fold 2  Fold 3  Fold 4 \n",
            "f1        0.6746  0.6817  0.6790  0.6750  0.6817\n",
            "cutoff    0.2946  0.2411  0.2970  0.2836  0.2242\n",
            "pctile    0.0721  0.0709  0.0735  0.0687  0.0719\n",
            "avg_prob  0.0437  0.0461  0.0434  0.0442  0.0465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kacKDuGQze30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "aab651cc-cdf4-47fc-c312-b8c0ba102026"
      },
      "source": [
        "oof_f1, oof_cutoff = f1_smart(oof[0],oof[1])\n",
        "print('OOF F1 = ',oof_f1,\" at \",oof_cutoff)\n",
        "oof_labels = pd.Series(oof[1] >= oof_cutoff, name=\"pred\")\n",
        "print(\"F1 Score at OOF Cutoff: {:7.4f}\".format(f1_score(oof[0],oof_labels)))\n",
        "print(pd.crosstab(pd.Series(oof[0],name=\"true\"), oof_labels))\n",
        "np.save(\"score.npy\",oof)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OOF F1 =  0.6767031275509015  at  0.27970461547374725\n",
            "F1 Score at OOF Cutoff:  0.6767\n",
            "pred    False  True \n",
            "true                \n",
            "0.0   1192644  32668\n",
            "1.0     22780  58030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tyb1v6Uze33"
      },
      "source": [
        "optimal_threshold = oof_cutoff\n",
        "#optimal_threshold = 0.34\n",
        "sub_['prediction'] = (y_test>optimal_threshold).astype(int)\n",
        "sub_.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}